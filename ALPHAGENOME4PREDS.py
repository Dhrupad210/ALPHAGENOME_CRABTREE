# -*- coding: utf-8 -*-
"""alphagen4feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VuTSRKfiJXrPR5noCeE6C7YywXtQDT49
"""

!pip install -q alphagenome biopython


import os
import pandas as pd
from Bio import SeqIO
from google.colab import drive
from alphagenome.models import dna_client
import numpy as np


api_key = 'AIxxxxxx'
dna_model = dna_client.create(api_key)


drive.mount('/content/drive')
folder_positive = '/content/drive/My Drive/alphagenome_proj/crabtree_positive-20240225T064530Z-001 (1)'
folder_negative = '/content/drive/My Drive/alphagenome_proj/crabtree_negative-20240225T064533Z-001 (1)'


def load_fasta_files(folder):
   sequences = []
   for file_name in os.listdir(folder):
       if file_name.endswith(('.fasta', '.fasta.txt', '.fa')):
           file_path = os.path.join(folder, file_name)
           for record in SeqIO.parse(file_path, "fasta"):
               sequences.append((record.id, str(record.seq).upper()))
   return sequences


positive_sequences = load_fasta_files(folder_positive)
negative_sequences = load_fasta_files(folder_negative)


print(f"Loaded {len(positive_sequences)} positive and {len(negative_sequences)} negative sequences.")

import pandas as pd
from Bio import SeqIO
from alphagenome.models import dna_client
import numpy as np
import os
import time




feature_output_types = [
   dna_client.OutputType.RNA_SEQ,
   dna_client.OutputType.CAGE,
   dna_client.OutputType.DNASE,
   dna_client.OutputType.CHIP_HISTONE
]


feature_ontology_terms = ['UBERON:0000955']
sequence_length = dna_client.SEQUENCE_LENGTH_2KB


def prepare_sequence(sequence):
   """Pads or trims a DNA sequence to the required length for the AlphaGenome model."""
   if len(sequence) < sequence_length:
       return sequence.center(sequence_length, 'N')
   elif len(sequence) > sequence_length:
       start = (len(sequence) - sequence_length) // 2
       return sequence[start:start + sequence_length]
   return sequence


def extract_features_from_sequence(seq_id, sequence):
   """
   Extracts AlphaGenome-based features for a single DNA sequence.
   Returns a dictionary of features.
   """
   padded_sequence = prepare_sequence(sequence)
   features = {'sequence_id': seq_id}


   # Exponential backoff for API calls to handle rate limits
   retries = 3
   for attempt in range(retries):
       try:
           output = dna_model.predict_sequence(
               sequence=padded_sequence,
               requested_outputs=feature_output_types,
               ontology_terms=feature_ontology_terms
           )
           break
       except Exception as e:
           if attempt < retries - 1:
               time.sleep(2**(attempt+1))
           else:
               print(f"Failed to get prediction for {seq_id} after {retries} attempts: {e}")

               for output_type in feature_output_types:
                   prefix = output_type.name.lower() + '_'
                   features[prefix + 'sum_pos'] = np.nan
                   features[prefix + 'sum_neg'] = np.nan
                   features[prefix + 'max_pos'] = np.nan
                   features[prefix + 'max_neg'] = np.nan
                   features[prefix + 'std_pos'] = np.nan
                   features[prefix + 'std_neg'] = np.nan
                   features[prefix + 'ratio_pos_neg'] = np.nan
                   features[prefix + 'diff_sum'] = np.nan
                   features[prefix + 'pos_max_pos'] = np.nan
                   features[prefix + 'pos_max_neg'] = np.nan
               return features


   for output_type in feature_output_types:
       track_data = getattr(output, output_type.name.lower())
       prediction_values = track_data.values


       if prediction_values.ndim == 2 and prediction_values.shape[1] >= 2:
           pos_strand_values = prediction_values[:, 0]
           neg_strand_values = prediction_values[:, 1]
           if prediction_values.shape[1] > 2:
               print(f"Note: {output_type.name} for {seq_id} has {prediction_values.shape[1]} tracks. Only the first two are used for pos/neg features.")
       elif prediction_values.ndim == 1 or prediction_values.shape[1] == 1:

           pos_strand_values = prediction_values.flatten()
           neg_strand_values = np.zeros_like(pos_strand_values)
       else:
           pos_strand_values = np.full(sequence_length, np.nan)
           neg_strand_values = np.full(sequence_length, np.nan)
           print(f"Warning: Unexpected track data shape for {output_type.name} in {seq_id}: {prediction_values.shape}. Filling with NaNs.")




       prefix = output_type.name.lower() + '_'
       features[prefix + 'sum_pos'] = np.sum(pos_strand_values) if not np.isnan(pos_strand_values).all() else np.nan
       features[prefix + 'sum_neg'] = np.sum(neg_strand_values) if not np.isnan(neg_strand_values).all() else np.nan
       features[prefix + 'max_pos'] = np.max(pos_strand_values) if not np.isnan(pos_strand_values).all() else np.nan
       features[prefix + 'max_neg'] = np.max(neg_strand_values) if not np.isnan(neg_strand_values).all() else np.nan
       features[prefix + 'std_pos'] = np.std(pos_strand_values) if not np.isnan(pos_strand_values).all() else np.nan
       features[prefix + 'std_neg'] = np.std(neg_strand_values) if not np.isnan(neg_strand_values).all() else np.nan
       denom = np.sum(neg_strand_values)
       if denom == 0 or np.isnan(denom) or np.isnan(np.sum(pos_strand_values)):
            features[prefix + 'ratio_pos_neg'] = np.nan
       else:
            features[prefix + 'ratio_pos_neg'] = np.sum(pos_strand_values) / (denom + 1e-9)


       features[prefix + 'diff_sum'] = np.sum(pos_strand_values) - np.sum(neg_strand_values) if not np.isnan(pos_strand_values).all() and not np.isnan(neg_strand_values).all() else np.nan
       features[prefix + 'pos_max_pos'] = np.argmax(pos_strand_values) if not np.isnan(pos_strand_values).all() else np.nan
       features[prefix + 'pos_max_neg'] = np.argmax(neg_strand_values) if not np.isnan(neg_strand_values).all() else np.nan
   return features
positive_features = [extract_features_from_sequence(seq_id, seq) for seq_id, seq in positive_sequences]
negative_features = [extract_features_from_sequence(seq_id, seq) for seq_id, seq in negative_sequences]


for f in positive_features:
   f['class_label'] = 'positive'
for f in negative_features:
   f['class_label'] = 'negative'
combined_features_df = pd.DataFrame(positive_features + negative_features)
combined_features_df = combined_features_df.fillna(0)
print("DataFrame of Extracted Features :")
print(combined_features_df)
print(f"\nTotal features columns: {len(combined_features_df.columns)}")

print(combined_features_df.describe())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import os

warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')
warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')

print("--- Starting ML Pivot Experiment with Multiple Models ---")
output_dir_ml = '/content/alphagenome_visualizations/ML_Results'
os.makedirs(output_dir_ml, exist_ok=True)
print(f"ML results (metrics and plots) will be saved to: {output_dir_ml}")


data_df = combined_features_df.copy()
data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)
X = data_df.drop(columns=['sequence_id', 'class_label'])
y = data_df['class_label'].map({'negative': 0, 'positive': 1})

print(f"Features (X) shape: {X.shape}")
print(f"Labels (y) shape: {y.shape}")
print(f"Label distribution after shuffling: \n{y.value_counts()}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)

print(f"\nTraining data size: {len(X_train)} samples")
print(f"Testing data size: {len(X_test)} samples")
print(f"Training label distribution: \n{y_train.value_counts()}")
print(f"Test label distribution: \n{y_test.value_counts()}")


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("\nFeatures scaled using StandardScaler.")

models = {
    'Logistic Regression': LogisticRegression(random_state=42, solver='liblinear'),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Support Vector Machine (SVC)': SVC(random_state=42, probability=True),
    'K-Nearest Neighbors (KNN)': KNeighborsClassifier(n_neighbors=3), # K=3 as a starting point
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
}

results = []


print("\n--- Model Training and Evaluation ---")
for name, model in models.items():
    print(f"\n--- Training and evaluating: {name} ---")


    model.fit(X_train_scaled, y_train)


    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else [0] * len(y_test)


    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)


    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision (Macro)': precision,
        'Recall (Macro)': recall,
        'F1-Score (Macro)': f1
    })


    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=['negative', 'positive'], zero_division=0))


    plt.figure(figsize=(6, 5))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Predicted Negative', 'Predicted Positive'],
                yticklabels=['Actual Negative', 'Actual Positive'])
    plt.title(f'Confusion Matrix for {name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plot_filename_cm = os.path.join(output_dir_ml, f'confusion_matrix_{name.replace(" ", "_")}.png')
    plt.savefig(plot_filename_cm, dpi=300)
    plt.close()


results_df = pd.DataFrame(results)
print("\n--- Summary of All Model Performances ---")
print(results_df.round(4))

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt # Still needed for overall summary if we add a plot later
import seaborn as sns # Still needed for overall summary if we add a plot later
from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold # Changed import
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer
import warnings

warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn') # For potential LogisticRegression/XGBoost warnings

print("--- Starting Hyperparameter Tuning for ALL ML Models ---")

data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)
X = data_df.drop(columns=['sequence_id', 'class_label'])
y = data_df['class_label'].map({'negative': 0, 'positive': 1})
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("\nFeatures scaled using StandardScaler.")


tuned_models = {
    'Logistic Regression': {
        'estimator': LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),
        'param_grid': {
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2']
        }
    },
    'Decision Tree': {
        'estimator': DecisionTreeClassifier(random_state=42),
        'param_grid': {
            'max_depth': [2, 3, 5, 7, 10],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2],
            'criterion': ['gini', 'entropy']
        }
    },
    'Random Forest': {
        'estimator': RandomForestClassifier(random_state=42),
        'param_grid': {
            'n_estimators': [50, 100, 150],
            'max_depth': [3, 5, 7],
            'min_samples_leaf': [1, 2],
            'criterion': ['gini', 'entropy']
        }
    },
    'Support Vector Machine (SVC)': {
        'estimator': SVC(random_state=42, probability=True),
        'param_grid': {
            'C': [0.1, 1, 10],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto']
        }
    },
    'K-Nearest Neighbors (KNN)': {
        'estimator': KNeighborsClassifier(),
        'param_grid': {
            'n_neighbors': [1, 3, 5, 7, 9],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
    },
    'Gradient Boosting': {
        'estimator': GradientBoostingClassifier(random_state=42),
        'param_grid': {
            'n_estimators': [50, 100, 150],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5],
            'subsample': [0.8, 1.0]
        }
    },
    'XGBoost': {
        'estimator': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
        'param_grid': {
            'n_estimators': [50, 100, 150],
            'max_depth': [3, 5],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 1.0],
        }
    }
}


cv_strategy = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=42)
scorer = make_scorer(f1_score, average='macro')

all_tuned_results = []

for name, config in tuned_models.items():
    print(f"\n--- Tuning {name} ---")

    grid_search = GridSearchCV(
        estimator=config['estimator'],
        param_grid=config['param_grid'],
        cv=cv_strategy,
        scoring=scorer,
        verbose=1,
        n_jobs=-1,
        return_train_score=True
    )

    grid_search.fit(X_train_scaled, y_train)
    best_model = grid_search.best_estimator_

    print(f"  Best parameters found for {name}: {grid_search.best_params_}")
    print(f"  Best cross-validation F1-score (Macro): {grid_search.best_score_:.4f}")


    best_index = grid_search.cv_results_['rank_test_score'].argmin()
    cv_f1_std = grid_search.cv_results_['std_test_score'][best_index]
    print(f"  Cross-validation F1-score (Macro) Std Dev: {cv_f1_std:.4f}")


    y_pred = best_model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)

    print(f"  Test Set Accuracy: {accuracy:.4f}")
    print(f"  Test Set Precision (Macro Average): {precision_macro:.4f}")
    print(f"  Test Set Recall (Macro Average): {recall_macro:.4f}")
    print(f"  Test Set F1-Score (Macro Average): {f1_macro:.4f}")


    all_tuned_results.append({
        'Model': name,
        'Best Params': grid_search.best_params_,
        'CV F1 Score (Macro Mean)': grid_search.best_score_,
        'CV F1 Score (Macro Std)': cv_f1_std,
        'Test Accuracy': accuracy,
        'Test Precision (Macro)': precision_macro,
        'Test Recall (Macro)': recall_macro,
        'Test F1-Score (Macro)': f1_macro
    })

# Summarize all tuned model performances
tuned_results_df = pd.DataFrame(all_tuned_results)
print("\n--- Summary of Tuned Model Performances ---")
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
print(tuned_results_df.round(4))
pd.reset_option('display.max_columns')
pd.reset_option('display.width')


best_model_row = tuned_results_df.loc[tuned_results_df['CV F1 Score (Macro Mean)'].idxmax()]
print("\n--- Overall Best Model based on Cross-Validation F1-Score (Macro Mean) ---")
print(f"Model: {best_model_row['Model']}")
print(f"Best Parameters: {best_model_row['Best Params']}")
print(f"Cross-Validation F1-Score (Macro Mean): {best_model_row['CV F1 Score (Macro Mean)']:.4f}")
print(f"Cross-Validation F1-Score (Macro Std): {best_model_row['CV F1 Score (Macro Std)']:.4f}")
print(f"Test Accuracy: {best_model_row['Test Accuracy']:.4f}")
print(f"Test F1-Score (Macro): {best_model_row['Test F1-Score (Macro)']:.4f}")
